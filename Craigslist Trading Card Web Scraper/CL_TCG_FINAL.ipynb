{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craigslist TCG Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search Imports\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pokemon Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Scrape Successful\n",
      "\n",
      "\n",
      "Pokemon Scrape Complete!\n"
     ]
    }
   ],
   "source": [
    "#Find the total number of posts to find the limit of the pagination\n",
    "response = get('https://newjersey.craigslist.org/search/sss?&query=pokemon+cards&sort=date&search_distance=10&postal=07030&hasPic=1')\n",
    "page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "results_num = page_html.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. \n",
    "#this np.arange function will step in size of 120 i.e. if result total is 400 then pages = array([  0, 120, 240, 360])\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "#See 'iterations += 1' towards end of script for details\n",
    "iterations = 0\n",
    "\n",
    "pokemon_post_timing = [] #create empty list for post dates\n",
    "pokemon_post_title_texts = [] #create empty list for post titles\n",
    "pokemon_post_locations = [] #create empty list for post locations\n",
    "pokemon_post_links = [] #create empty list for post urls\n",
    "pokemon_post_prices = [] #create empty list for post prices\n",
    "    \n",
    "#for each page of search results, do the following\n",
    "for page in pages:\n",
    "    \n",
    "    #get request for each page - note how str(page) will take each page from pages \n",
    "    response = get(\"https://newjersey.craigslist.org/search/sss?\"\n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&query=pokemon+cards&sort=date&search_distance=10&postal=07030&hasPic=1\")\n",
    "\n",
    "    #sleep random number of seconds between 1 and 5 before sending another request to avoid being blocked if too many requests are being sent\n",
    "    sleep(randint(1,5))\n",
    "\n",
    "    #throw warning for status codes that are not 200\n",
    "    #HTTP 200 OK success status response code indicates that the request has succeeded\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = page_html.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "        \n",
    "        #if the post doesn't list a neighborhood, take the following actions\n",
    "        if post.find('span', class_ = 'result-hood') is None:\n",
    "            \n",
    "            #Post Date\n",
    "            #Grab datetime element and append to list\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            pokemon_post_timing.append(post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            #Grab title and append to list\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text.title()\n",
    "            pokemon_post_title_texts.append(post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            #Grab post url and append to list\n",
    "            post_link = post_title['href']\n",
    "            pokemon_post_links.append(post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #Remove whitespace and currency symbol, change from string to integer, and remove extra spaces and append to list\n",
    "            post_price = int(post.a.text.replace(\"$\",\"\").replace(\",\",\"\").strip())\n",
    "            pokemon_post_prices.append(post_price)\n",
    "           \n",
    "            #If post has a location then do the following \n",
    "            if post.find('span', class_='nearby') is not None:\n",
    "            \n",
    "                #Post location exists\n",
    "                #Remove () around location text and change \"Nyc\" to \"NYC\" and append to list\n",
    "                post_location = post.find('span', class_='nearby').text.replace(\"(\", \"\").replace(\")\", \"\").title().replace(\"Nyc\", \"NYC\")\n",
    "                pokemon_post_locations.append(post_location)\n",
    "                \n",
    "                #Post location DOES NOT exist\n",
    "                #Append 'N/A' to list\n",
    "            else:\n",
    "                post_location = 'N/A'\n",
    "                pokemon_post_locations.append(post_location)\n",
    "                \n",
    "        #OTHERWISE, if the listing does have the neighborhood data available then do the following     \n",
    "        elif post.find('span', class_ = 'result-hood') is not None:\n",
    "            \n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            pokemon_post_timing.append(post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text.title()\n",
    "            pokemon_post_title_texts.append(post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            post_link = post_title['href']\n",
    "            pokemon_post_links.append(post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.replace(\"$\",\"\").replace(\",\",\"\").strip())\n",
    "            pokemon_post_prices.append(post_price)\n",
    "           \n",
    "            #Post Neighborhood\n",
    "            post_hood = post.find('span', class_= 'result-hood').text.replace(\"(\", \"\").replace(\")\", \"\").title().strip()\n",
    "            pokemon_post_locations.append(post_hood)\n",
    "            \n",
    "    #For each loop (page in pages) print 'Page X Scrape Successful'\n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" Scrape Successful\")\n",
    "\n",
    "#Add space between 'Page x Scrape Successful' and \"Pokemon Scrape Complete!\"\n",
    "print(\"\\n\")\n",
    "\n",
    "#Once all pages in the search have been scraped, print \"Pokemon Scrape Complete!\"\n",
    "print(\"Pokemon Scrape Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic The Gathering Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Scrape Successful\n",
      "\n",
      "\n",
      "MTG Scrape Complete!\n"
     ]
    }
   ],
   "source": [
    "#Find the total number of posts to find the limit of the pagination\n",
    "MTG_response = get('https://newjersey.craigslist.org/search/sss?&query=magic+the+gathering&sort=date&search_distance=10&postal=07030&hasPic=1')\n",
    "MTG_page_html = BeautifulSoup(MTG_response.text, 'html.parser')\n",
    "MTG_results_num = MTG_page_html.find('div', class_= 'search-legend')\n",
    "MTG_results_total = int(MTG_results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. \n",
    "#this np.arange function will step in size of 120 i.e. if result total is 400 then pages = array([  0, 120, 240, 360])\n",
    "MTG_pages = np.arange(0, MTG_results_total+1, 120)\n",
    "\n",
    "#See 'iterations += 1' towards end of script for details\n",
    "MTG_iterations = 0\n",
    "\n",
    "MTG_post_timing = [] #create empty list for post dates\n",
    "MTG_post_title_texts = [] #create empty list for post titles\n",
    "MTG_post_locations = [] #create empty list for post locations\n",
    "MTG_post_links = [] #create empty list for post urls\n",
    "MTG_post_prices = [] #create empty list for post prices\n",
    "\n",
    "#for each page of search results, do the following\n",
    "for MTG_page in MTG_pages:\n",
    "    \n",
    "    #get request for each page - note how str(page) will take each page from pages \n",
    "    MTG_response = get(\"https://newjersey.craigslist.org/search/sss?\"\n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(MTG_page) #the page number in the pages array from earlier\n",
    "                   + \"&query=magic+the+gathering&sort=date&search_distance=10&postal=07030&hasPic=1\")\n",
    "    \n",
    "    #sleep random number of seconds between 1 and 5 before sending another request to avoid being blocked if too many requests are being sent\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    #HTTP 200 OK success status response code indicates that the request has succeeded\n",
    "    if MTG_response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    MTG_page_html = BeautifulSoup(MTG_response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    MTG_posts = MTG_page_html.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for MTG_post in MTG_posts:\n",
    "        \n",
    "        #if the post doesn't list a neighborhood, take the following actions\n",
    "        if MTG_post.find('span', class_ = 'result-hood') is None:\n",
    "            \n",
    "            #Post Date\n",
    "            #Grab datetime element and append to list\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            MTG_post_datetime = MTG_post.find('time', class_= 'result-date')['datetime']\n",
    "            MTG_post_timing.append(MTG_post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            #Grab title and append to list\n",
    "            MTG_post_title = MTG_post.find('a', class_='result-title hdrlnk')\n",
    "            MTG_post_title_text = MTG_post_title.text.title()\n",
    "            MTG_post_title_texts.append(MTG_post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            #Grab post url and append to list\n",
    "            MTG_post_link = MTG_post_title['href']\n",
    "            MTG_post_links.append(MTG_post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #Remove whitespace and currency symbol, change from string to integer, and remove extra spaces and append to list\n",
    "            MTG_post_price = int(post.a.text.strip().replace(\"$\", \"\").replace(\",\", \"\"))\n",
    "            MTG_post_prices.append(MTG_post_price)\n",
    "            \n",
    "            #If post has a location then do the following \n",
    "            if MTG_post.find('span', class_='nearby') is not None:\n",
    "            \n",
    "                #Post location exists\n",
    "                #Remove () around location text and change \"Nyc\" to \"NYC\" and append to list\n",
    "                MTG_post_location = MTG_post.find('span', class_='nearby').text.replace(\"(\", \"\").replace(\")\", \"\").title().replace(\"Nyc\", \"NYC\")\n",
    "                MTG_post_locations.append(MTG_post_location)\n",
    "                \n",
    "                #Post location DOES NOT exist\n",
    "                #Append 'N/A' to list\n",
    "            else:\n",
    "                MTG_post_location = 'N/A'\n",
    "                MTG_post_locations.append(MTG_post_location)\n",
    "                \n",
    "        #OTHERWISE, if the listing does have the neighborhood data available then do the following     \n",
    "        elif MTG_post.find('span', class_ = 'result-hood') is not None:\n",
    "            \n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            MTG_post_datetime = MTG_post.find('time', class_= 'result-date')['datetime']\n",
    "            MTG_post_timing.append(MTG_post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            MTG_post_title = MTG_post.find('a', class_='result-title hdrlnk')\n",
    "            MTG_post_title_text = MTG_post_title.text.title()\n",
    "            MTG_post_title_texts.append(MTG_post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            MTG_post_link = MTG_post_title['href']\n",
    "            MTG_post_links.append(MTG_post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            MTG_post_price = int(MTG_post.a.text.strip().replace(\"$\", \"\").replace(\",\",\"\")) \n",
    "            MTG_post_prices.append(MTG_post_price)\n",
    "           \n",
    "            #Post Neighborhood\n",
    "            MTG_post_hood = MTG_post.find('span', class_= 'result-hood').text.replace(\"(\", \"\").replace(\")\", \"\").title().strip()\n",
    "            MTG_post_locations.append(MTG_post_hood)\n",
    "            \n",
    "    #For each loop (page in pages) print 'Page X Scrape Successful'\n",
    "    MTG_iterations += 1\n",
    "    print(\"Page \" + str(MTG_iterations) + \" Scrape Successful\")\n",
    "\n",
    "#Add space between 'Page x Scrape Successful' and \"Pokemon Scrape Complete!\"\n",
    "print(\"\\n\")\n",
    "\n",
    "#Once all pages in the search have been scraped, print \"MTG Scrape Complete!\"\n",
    "print(\"MTG Scrape Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yu-Gi-Oh! Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Scrape Successful\n",
      "\n",
      "\n",
      "Yugioh Scrape Complete!\n"
     ]
    }
   ],
   "source": [
    "#Find the total number of posts to find the limit of the pagination\n",
    "Yugioh_response = get('https://newjersey.craigslist.org/search/sss?&query=yugioh&sort=date&search_distance=10&postal=07030&hasPic=1')\n",
    "Yugioh_page_html = BeautifulSoup(Yugioh_response.text, 'html.parser')\n",
    "Yugioh_results_num = Yugioh_page_html.find('div', class_= 'search-legend')\n",
    "Yugioh_results_total = int(Yugioh_results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. \n",
    "#this np.arange function will step in size of 120 i.e. if result total is 400 then pages = array([  0, 120, 240, 360])\n",
    "Yugioh_pages = np.arange(0, Yugioh_results_total+1, 120)\n",
    "\n",
    "#See 'iterations += 1' towards end of script for details\n",
    "Yugioh_iterations = 0\n",
    "\n",
    "Yugioh_post_timing = [] #create empty list for post dates\n",
    "Yugioh_post_title_texts = [] #create empty list for post titles\n",
    "Yugioh_post_locations = [] #create empty list for post locations\n",
    "Yugioh_post_links = [] #create empty list for post urls\n",
    "Yugioh_post_prices = [] #create empty list for post prices\n",
    "\n",
    "#for each page of search results, do the following\n",
    "for Yugioh_page in Yugioh_pages:\n",
    "    \n",
    "    #get request for each page - note how str(page) will take each page from pages \n",
    "    Yugioh_response = get(\"https://newjersey.craigslist.org/search/sss?\"\n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(Yugioh_page) #the page number in the pages array from earlier\n",
    "                   + \"&query=yugioh&sort=date&search_distance=10&postal=07030&hasPic=1\")\n",
    "\n",
    "    #sleep random number of seconds between 1 and 5 before sending another request to avoid being blocked if too many requests are being sent\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    #HTTP 200 OK success status response code indicates that the request has succeeded\n",
    "    if Yugioh_response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    Yugioh_page_html = BeautifulSoup(Yugioh_response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    Yugioh_posts = Yugioh_page_html.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for Yugioh_post in Yugioh_posts:\n",
    "        \n",
    "        #if the post doesn't list a neighborhood, take the following actions\n",
    "        if Yugioh_post.find('span', class_ = 'result-hood') is None:\n",
    "            \n",
    "            #Post Date\n",
    "            #Grab datetime element and append to list\n",
    "            Yugioh_post_datetime = Yugioh_post.find('time', class_= 'result-date')['datetime']\n",
    "            Yugioh_post_timing.append(Yugioh_post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            #Grab title and append to list\n",
    "            Yugioh_post_title = Yugioh_post.find('a', class_='result-title hdrlnk')\n",
    "            Yugioh_post_title_text = Yugioh_post_title.text.title()\n",
    "            Yugioh_post_title_texts.append(Yugioh_post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            #Grab post url and append to list\n",
    "            Yugioh_post_link = Yugioh_post_title['href']\n",
    "            Yugioh_post_links.append(Yugioh_post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #Remove whitespace and currency symbol, change from string to integer, and remove extra spaces and append to list\n",
    "            Yugioh_post_price = int(Yugioh_post.a.text.strip().replace(\"$\", \"\").replace(\",\",\"\")) \n",
    "            Yugioh_post_prices.append(Yugioh_post_price)\n",
    "           \n",
    "            #If post has a location then do the following \n",
    "            if Yugioh_post.find('span', class_='nearby') is not None:\n",
    "            \n",
    "                #Post location exists\n",
    "                #Remove () around location text and change \"Nyc\" to \"NYC\" and append to list\n",
    "                Yugioh_post_location = Yugioh_post.find('span', class_='nearby').text.replace(\"(\", \"\").replace(\")\", \"\").title().replace(\"Nyc\", \"NYC\")\n",
    "                Yugioh_post_locations.append(Yugioh_post_location)\n",
    "                \n",
    "                #Post location DOES NOT exist\n",
    "                #Append 'N/A' to list\n",
    "            else:\n",
    "                Yugioh_post_location = 'N/A'\n",
    "                Yugioh_post_locations.append(MTG_post_location)\n",
    "\n",
    "        #OTHERWISE, if the listing does have the neighborhood data available then do the following     \n",
    "        elif Yugioh_post.find('span', class_ = 'result-hood') is not None:\n",
    "            \n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            Yugioh_post_datetime = Yugioh_post.find('time', class_= 'result-date')['datetime']\n",
    "            Yugioh_post_timing.append(Yugioh_post_datetime)\n",
    "            \n",
    "            #Post Title\n",
    "            Yugioh_post_title = Yugioh_post.find('a', class_='result-title hdrlnk')\n",
    "            Yugioh_post_title_text = Yugioh_post_title.text.title()\n",
    "            Yugioh_post_title_texts.append(Yugioh_post_title_text)\n",
    "\n",
    "            #Post URL\n",
    "            Yugioh_post_link = Yugioh_post_title['href']\n",
    "            Yugioh_post_links.append(Yugioh_post_link)\n",
    "            \n",
    "            #Post Price\n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            Yugioh_post_price = int(post.a.text.strip().replace(\"$\", \"\").replace(\",\", \"\"))\n",
    "            Yugioh_post_prices.append(Yugioh_post_price)\n",
    "           \n",
    "            #Post Neighborhood\n",
    "            Yugioh_post_hood = Yugioh_post.find('span', class_= 'result-hood').text.replace(\"(\", \"\").replace(\")\", \"\").title().strip()\n",
    "            Yugioh_post_locations.append(Yugioh_post_hood)\n",
    "            \n",
    "    #For each loop (page in pages) print 'Page X Scrape Successful'\n",
    "    Yugioh_iterations += 1\n",
    "    print(\"Page \" + str(Yugioh_iterations) + \" Scrape Successful\")\n",
    "\n",
    "#Add space between 'Page x Scrape Successful' and \"Pokemon Scrape Complete!\"\n",
    "print(\"\\n\")\n",
    "\n",
    "#Once all pages in the search have been scraped, print \"Yugioh Scrape Complete!\"\n",
    "print(\"Yugioh Scrape Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Frame From Scrape Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Imports\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pokemon Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "pokemon_card_data = pd.DataFrame({'POSTED DATE': pokemon_post_timing,\n",
    "                        'POST TITLE': pokemon_post_title_texts,\n",
    "                        'POST LOCATION': pokemon_post_locations,\n",
    "                        'PRICE': pokemon_post_prices,\n",
    "                        'URL': pokemon_post_links,})\n",
    "\n",
    "#Add a column called 'TCG' to the DF and for each row and add 'Pokemon'\n",
    "pokemon_card_data['TCG'] = 'Pokemon'\n",
    "pokemon_card_data['TCG'] = pokemon_card_data['TCG'].astype(str)\n",
    "\n",
    "#Change post date to be in correct datetime format\n",
    "pokemon_card_data['POSTED DATE']=pokemon_card_data['POSTED DATE'].astype('datetime64[ns]')\n",
    "pokemon_card_data['POSTED DATE']=pokemon_card_data['POSTED DATE'].dt.strftime(\"%m/%d/%y %I:%M %p\")\n",
    "\n",
    "#Reorder Columns\n",
    "pokemon_card_data = pokemon_card_data[['POSTED DATE','TCG','POST TITLE','POST LOCATION','PRICE','URL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTG Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "MTG_card_data = pd.DataFrame({'POSTED DATE': MTG_post_timing,\n",
    "                        'POST TITLE': MTG_post_title_texts,\n",
    "                        'POST LOCATION': MTG_post_locations,\n",
    "                        'PRICE': MTG_post_prices,\n",
    "                        'URL': MTG_post_links,})\n",
    "\n",
    "#Add a column called 'TCG' to the DF and for each row and add 'MTG'\n",
    "MTG_card_data['TCG'] = 'MTG'\n",
    "MTG_card_data['TCG'] = MTG_card_data['TCG'].astype(str)\n",
    "\n",
    "#Change post date to be in correct datetime format\n",
    "MTG_card_data['POSTED DATE']=MTG_card_data['POSTED DATE'].astype('datetime64[ns]')\n",
    "MTG_card_data['POSTED DATE']=MTG_card_data['POSTED DATE'].dt.strftime(\"%m/%d/%y %I:%M %p\")\n",
    "\n",
    "#Reorder Columns\n",
    "MTG_card_data = MTG_card_data[['POSTED DATE','TCG','POST TITLE','POST LOCATION','PRICE','URL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yu-Gi-Oh! Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "Yugioh_card_data = pd.DataFrame({'POSTED DATE': Yugioh_post_timing,\n",
    "                        'POST TITLE': Yugioh_post_title_texts,\n",
    "                        'POST LOCATION': Yugioh_post_locations,\n",
    "                        'PRICE': Yugioh_post_prices,\n",
    "                        'URL': Yugioh_post_links,})\n",
    "\n",
    "#Add a column called 'TCG' to the DF and for each row and add 'Yugioh'\n",
    "Yugioh_card_data['TCG'] = 'Yugioh'\n",
    "Yugioh_card_data['TCG'] = Yugioh_card_data['TCG'].astype(str)\n",
    "\n",
    "#Change post date to be in correct datetime format\n",
    "Yugioh_card_data['POSTED DATE']=Yugioh_card_data['POSTED DATE'].astype('datetime64[ns]')\n",
    "Yugioh_card_data['POSTED DATE']=Yugioh_card_data['POSTED DATE'].dt.strftime(\"%m/%d/%y %I:%M %p\")\n",
    "\n",
    "#Reorder Columns\n",
    "Yugioh_card_data = Yugioh_card_data[['POSTED DATE','TCG','POST TITLE','POST LOCATION','PRICE','URL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCG_Table = pd.concat([pokemon_card_data, MTG_card_data, Yugioh_card_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Email Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email():\n",
    "    \n",
    "    #Imports\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.application import MIMEApplication\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from smtplib import SMTP\n",
    "    from datetime import datetime\n",
    "    import smtplib\n",
    "    import sys\n",
    "\n",
    "    #Get date/time and set subject\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%B %d, %Y %I:%M %p\")\n",
    "    subject = \"Craigslist TCG Scraper - {}\".format(dt_string) #Set subject of email here and include datetime of when email was sent\n",
    "\n",
    "    #set email fields\n",
    "    recipients = ['brandon.levan1014@gmail.com'] #Add recipient's email address here. If more than one, comma seperate.\n",
    "    emaillist = [elem.strip().split(',') for elem in recipients]\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = subject #Will read from subject variable defined above\n",
    "    msg['From'] = 'blevan.develops@gmail.com' #Add sender's email address here\n",
    "\n",
    "    #set body of email\n",
    "    html = \"\"\"\\\n",
    "    <html>\n",
    "      <head></head>\n",
    "      <body>\n",
    "        {0}\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\".format(new_listings_updated.to_html(justify='left')) #Will send new_listings_updated dataframe in the body of the email\n",
    "\n",
    "    part1 = MIMEText(html, 'html')\n",
    "    msg.attach(part1)\n",
    "\n",
    "    #Gmail server configs\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "    server.login(CONF['usrnm'], CONF['pd']) #Reads in same username and password as I set in cl_config.text from db step\n",
    "    server.sendmail(msg['From'], emaillist , msg.as_string())\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DB Write Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that does the following\n",
    "   #Read username and password from local file in order to have access to write to table\n",
    "      #I manually created a schema in MySQL database called 'Craigslist' and have a table named 'TCG' for this function to write to\n",
    "   #Function will write new_listings_updated to the db\n",
    "\n",
    "def write_to_DB():\n",
    "    ##import u/p from config file stored in directory\n",
    "    from collections import defaultdict\n",
    "    FILE = open(\"/Users/brandonlevan/Desktop/Craigslist/cl_config.txt\",\"r\")\n",
    "    CONF = defaultdict(str)\n",
    "    for line in FILE:\n",
    "        conf_data = line.strip().split('=')\n",
    "        CONF[conf_data[0].strip()] = conf_data[1].strip()\n",
    "\n",
    "    #Import that allows to connection to db\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    #Create sqlalchemy engine - Pass configs to engine to connect to db\n",
    "    engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                           .format(user=\"root\",\n",
    "                                   pw= CONF['pd'],\n",
    "                                   db=\"Craigslist\"))\n",
    "\n",
    "    #Write new_listings_updated to database\n",
    "    new_listings_updated.to_sql('TCG', con = engine , if_exists = 'append', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Old Listings From Previous Scrape and Check For New Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Writing to Database and Sending Email Report of New Listings\n"
     ]
    }
   ],
   "source": [
    "#Connect to DB\n",
    "#Read in u/p from config file\n",
    "from collections import defaultdict\n",
    "FILE = open(\"/Users/brandonlevan/Desktop/Craigslist/cl_config.txt\",\"r\")\n",
    "CONF = defaultdict(str)\n",
    "for line in FILE:\n",
    "    conf_data = line.strip().split('=')\n",
    "    CONF[conf_data[0].strip()] = conf_data[1].strip()\n",
    "\n",
    "#Import that allows to connection to db\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Create sqlalchemy engine - Pass configs to engine to connect to db\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"root\",\n",
    "                               pw= CONF['pd'],\n",
    "                               db=\"Craigslist\"))\n",
    "\n",
    "#Read in data from TCG table store on MySQL databse uisng SQL statement\n",
    "df1 = pd.read_sql_query(\"SELECT * FROM TCG\", engine)\n",
    "\n",
    "#Drop posted date from table\n",
    "df1 = df1.drop(columns=['index', 'POSTED DATE'])\n",
    "\n",
    "#From our combined TCG table we got from our CL scrape, drop posted date\n",
    "df2 = TCG_Table.drop(columns=['POSTED DATE'])\n",
    "\n",
    "#Combine scrape table and old posting table and drop duplicates\n",
    "new_listings = pd.concat([df1,df2]).drop_duplicates(keep=False)\n",
    "\n",
    "#Merge de-duplicated table with latest listings table to get the most recent dates\n",
    "new_listings_updated = TCG_Table.merge(new_listings, on=['TCG','POST TITLE','POST LOCATION','PRICE','URL'])\n",
    "\n",
    "#Set post date to be the index of the date and sort new to old. Change price to be in currency format\n",
    "new_listings_updated = new_listings_updated.set_index('POSTED DATE').sort_index(ascending=False)#.sort_values(by='POSTED DATE', ascending=False)\n",
    "new_listings_updated['PRICE'] = new_listings_updated['PRICE'].apply(lambda x: \"${:,.2f}\".format((x)))\n",
    "\n",
    "#If there are new listing, write listing to DB and send email with new listing otherwise don't do anything\n",
    "if new_listings_updated.empty is True:\n",
    "    print('No New Listings')\n",
    "\n",
    "#ELSE RUN EMAIL REPORT AND WRITE TO DB\n",
    "    \n",
    "else:\n",
    "    print('Now Writing to Database and Sending Email Report of New Listings')\n",
    "    write_to_DB()\n",
    "    send_email()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cron Jon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set up Cron Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To set up your cron job\n",
    "\n",
    "#Open terminal and type, hit enter\n",
    "#crontab -e \n",
    "\n",
    "#Next type i to allow you to insert the cron job. Type i then hit enter\n",
    "#i \n",
    "\n",
    "#Paste the following cronjob\n",
    "#This will run your python script every 6 hours when connected to the internet\n",
    "#If you are not connected to wifi, the script will not fully run because will not scrape CL or send email\n",
    "#0 */6 * * * /opt/anaconda3/bin/python /Users/brandonlevan/Desktop/Craigslist/name_of_your_file.py\n",
    "\n",
    "#To escape crontab\n",
    "#esc / :wp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
